# Descripci칩n del Dataset de Retrasos y Cancelaciones de Vuelos
## (20242Q) MLE.01 - Machine Learning Engineering - Cloud Data Engineering ITBA 2024

### Alumnos:

*   D. Nicol치s Morelli
*   Facundo G. Argibay
*   Lucas Franco
*   Jos칠 Ignacio L칩pez S치ez

## Contexto del Negocio

Este dataset aborda el problema de los retrasos en los vuelos, un desaf칤o significativo tanto para aerol칤neas como para pasajeros. Los retrasos impactan en costos operativos, satisfacci칩n del cliente y la eficiencia general de los aeropuertos. Para las aerol칤neas y organismos de aviaci칩n, poder predecir con precisi칩n la probabilidad de retraso de un vuelo es esencial para mejorar la experiencia del pasajero y la optimizaci칩n de recursos.

La variable objetivo en este an치lisis es **DEP_DEL15**, un valor binario que indica si un vuelo se retras칩 15 minutos o m치s en su partida. La meta es utilizar esta variable para entrenar modelos de machine learning que puedan predecir futuros retrasos en los vuelos.

## Informaci칩n del Dataset

Este dataset contiene informaci칩n detallada de vuelos, condiciones meteorol칩gicas, datos del aeropuerto y caracter칤sticas de la aerol칤nea para el a침o 2019. Incluye datos mensuales y variables relacionadas con el clima y con la operaci칩n de los aeropuertos, y adem치s permite realizar ajustes en el dataset, como agregar razones de cancelaci칩n y retrasos de llegada para problemas multicategor칤a.

| Caracter칤stica                    | Descripci칩n                                                                                                                                                                 |
|-----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Clasificaci칩n del problema**    | Clasificaci칩n binaria (evaluando retraso en la salida)                                                                                                                      |
| **Total de registros**            | 6,489,062 registros                                                                                                                                                        |
| **Total de columnas**             | 26 columnas, que abarcan variables num칠ricas, categ칩ricas y de tipo `float`                                                                                                |
| **Variables meteorol칩gicas**      | Incluyen precipitaciones (PRCP), nieve (SNOW), cobertura de nieve (SNWD), temperatura m치xima (TMAX), velocidad del viento (AWND)                                           |
| **Variables de aerol칤nea y vuelo**| Datos de cantidad de vuelos por mes, aerol칤nea, aeropuerto, segmento del vuelo, grupo de distancia, entre otros                                                            |
| **Variables de personal**         | Cantidad promedio de asistentes de vuelo por pasajero (FLT_ATTENDANTS_PER_PASS) y personal de tierra por pasajero (GROUND_SERV_PER_PASS)                                    |
| **Variables geogr치ficas**         | Aeropuertos de origen y destino, latitud y longitud de los aeropuertos de salida                                                                                           |

El archivo original del dataset est치 disponible en Kaggle bajo el nombre [2019 Airline Delays and Cancellations](https://www.kaggle.com/datasets/threnjen/2019-airline-delays-and-cancellations/data).

## Objetivo del Proyecto

El objetivo es entrenar modelos predictivos para anticipar la variable binaria **DEP_DEL15**. El desarrollo y la implementaci칩n del modelo se llevar치n a cabo en **Amazon SageMaker** en AWS, donde se buscar치 construir y optimizar un modelo de machine learning que permita prever la probabilidad de retrasos en vuelos y, eventualmente, generar insights pr치cticos para la toma de decisiones en la gesti칩n de operaciones de aerol칤neas y aeropuertos.

---

## Pasos Seguidos

### 1. Desarrollo Local
- Se cre칩 un pipeline de Data Science inicial utilizando **Pandas**, **XGBoost** y **TensorFlow**. 
- El pipeline fue dise침ado para abarcar desde el preprocesamiento de datos hasta el ajuste de hiperpar치metros.
- Tras probar exitosamente el proceso en un entorno local, se procedi칩 a adaptarlo a la infraestructura de AWS para mejorar su escalabilidad y rendimiento.

### 2. Exploraci칩n de Datos
- Un **Dashboard de EDA** se gener칩 para visualizar tendencias y comportamientos clave en los datos >> [2019 Airline Delays and Cancellations - Dashboard](https://nachols1986.github.io/infovis/airport_delays.html).
- Se identificaron caracter칤sticas relevantes para alimentar los modelos predictivos.

---

## Infraestructura en AWS

### 1. Almacenamiento en S3

- El dataset original, en formato CSV, fue almacenado en un bucket S3 denominado `data-bucket-itba-delays`. Esta configuraci칩n permite un acceso seguro y escalable para el procesamiento y modelado de datos.
- En un contexto real, para optimizar costos, podr칤a configurarse una pol칤tica de almacenamiento **Glacier** en S3. Esto ser칤a ideal para datos hist칩ricos que no requieran acceso frecuente, como datasets utilizados 칰nicamente para auditor칤as o an치lisis retrospectivos.
- A futuro, si se dispusiera de datos en tiempo real que incluyeran todas las variables necesarias, se podr칤a implementar un flujo de **ingesta continua**. Esto permitir칤a alimentar un pipeline que:
  - Integre datos en tiempo real a trav칠s de servicios como **Kinesis Data Streams** o **AWS IoT Core**.
  - Procese los datos entrantes utilizando **AWS Lambda** o **AWS Glue Streaming ETL**.
  - Almacene los datos preprocesados en formato **parquet** o **delta** para garantizar eficiencia en consultas y modelado.
- Este esquema permitir칤a desarrollar un **reentrenamiento peri칩dico del modelo**, donde:
  - Los datos en tiempo real se acumulen en el bucket de S3.
  - Un job automatizado de **Glue o Sagemaker Pipelines** se encargue de realizar el preprocesamiento, ajuste de hiperpar치metros y reentrenamiento del modelo seg칰n intervalos definidos.
  - Los modelos actualizados sean desplegados autom치ticamente como endpoints en Sagemaker, asegurando que las predicciones reflejen patrones recientes de comportamiento en los vuelos.

### 2. Roles IAM

- **GlueRole:** Este rol se cre칩 con permisos completos para acceder al bucket S3, permitiendo tanto la lectura del archivo CSV original como la escritura del archivo en formato **parquet** generado tras el preprocesamiento. Estos permisos son fundamentales para que **AWS Glue** pueda ejecutar los jobs de transformaci칩n de datos y registrar la tabla procesada en el cat치logo de Glue. 
  - **Ampliaci칩n futura:** En un entorno de producci칩n, se podr칤a optimizar la pol칤tica de permisos asignada limitando el acceso a rutas espec칤ficas del bucket o usando **condiciones basadas en etiquetas**. Por ejemplo, restringir el acceso 칰nicamente a archivos que contengan una etiqueta `dataset:flights`.
  - **Rotaci칩n de roles:** Tambi칠n se podr칤an implementar procesos autom치ticos para revisar y rotar las claves de acceso asociadas peri칩dicamente, cumpliendo con pol칤ticas de seguridad organizacionales.

- **SMRole:** Este rol se dise침칩 para otorgar permisos de **solo lectura** al bucket S3, permitiendo que los notebooks y jobs en **Sagemaker** ingieran el dataset procesado y contin칰en con el pipeline de modelado. Esto asegura que los recursos de Sagemaker puedan consumir datos necesarios sin exponer permisos innecesarios.
  - **Ampliaci칩n futura:** En un flujo m치s avanzado, este rol podr칤a integrarse con **Sagemaker Feature Store**, permitiendo la lectura no solo de datasets completos, sino tambi칠n de **caracter칤sticas espec칤ficas** preprocesadas y almacenadas en Feature Store, optimizando el uso de recursos.

- **Buenas pr치cticas y ampliaciones generales:**
  - **Principio de menor privilegio:** Se recomienda siempre restringir permisos al m칤nimo necesario. Por ejemplo, se podr칤a usar una pol칤tica espec칤fica que limite las acciones a `s3:GetObject` y `s3:PutObject` para las rutas necesarias del bucket.
  - **Monitoreo y auditor칤a:** Se pueden activar logs de acceso con **AWS CloudTrail** para rastrear el uso de estos roles, identificar patrones sospechosos y garantizar que no se usen para actividades no autorizadas.
  - **Integraci칩n con otros servicios:** Ambos roles podr칤an ampliarse para colaborar con servicios adicionales. Por ejemplo:
    - **EventBridge** para orquestar eventos basados en el flujo de datos.
    - **Athena** para ejecutar consultas SQL directamente sobre el parquet generado por Glue, reduciendo la necesidad de mover datos entre servicios.
    - **KMS (AWS Key Management Service)** para cifrar tanto el bucket como las comunicaciones entre servicios, asegurando el cumplimiento de normativas.

### 3. AWS Glue
- El preprocesamiento fue migrado a **AWS Glue**, adaptando los pasos realizados en Pandas a **PySpark**.
- Incluye:
  - Creaci칩n de nuevas columnas (como estaciones seg칰n latitud y mes).
  - Escalado y transformaci칩n de variables.
- El procesamiento distribuido de Glue permite manejar grandes vol칰menes de datos de manera eficiente.
- Para m치s detalles, consulte el Notebook de Glue.

### 4. AWS Sagemaker
- Los datos preprocesados se consumieron desde Athena utilizando la tabla catalogada en Glue.
- Se mantuvo el pipeline local de entrenamiento y ajuste, que prob칩 m칰ltiples algoritmos y seleccion칩 los mejores.
- Se despleg칩 el endpoint del modelo basado en red neuronal en Sagemaker para su utilizaci칩n.

---

## Conclusiones

- El modelo m치s efectivo fue **XGBoost**, debido a su robustez frente a outliers presentes en las columnas del dataset.
- Resultados principales:
  - **XGBoost:** AUC = **0.9356**
  - **Red Neuronal:** AUC = **0.6779**
- Si bien la red neuronal mostr칩 resultados inferiores, el pipeline est치 dise침ado para adaptarse a nuevas iteraciones y mejorar el rendimiento en el futuro.

---

## Pr칩ximos Pasos

- Implementar almacenamiento de largo plazo en Glacier para optimizar costos.
- Desarrollar un flujo automatizado para reentrenamiento peri칩dico de modelos.
- Experimentar con t칠cnicas avanzadas de limpieza de datos para mitigar el impacto de outliers.

춰Gracias por revisar nuestro proyecto! 游
